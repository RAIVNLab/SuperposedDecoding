{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/raivn/ethans/miniconda3/envs/llms_12.1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 72 from C header, got 88 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow._fs.FileInfo size changed, may indicate binary incompatibility. Expected 64 from C header, got 88 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow._fs.FileSelector size changed, may indicate binary incompatibility. Expected 48 from C header, got 72 from PyObject\n",
      "2024-05-27 20:15:02.245095: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-27 20:15:02.294117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 20:15:06.982179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval import *\n",
    "from superposed.llama.metrics import *\n",
    "from superposed.llama.generation import Llama\n",
    "from superposed.llama.superposed_generation import SuperposedLlama\n",
    "from superposed.llama.tokenizer import Tokenizer\n",
    "from superposed.ngrams.ngram_models import make_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(\"../gpt-2-output-dataset/data/webtext.test.jsonl\", \"r\") as f:\n",
    "    dataset = [json.loads(line)[\"text\"] for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'alpha': 0.54, 'temp': 0.06, 'n_drafts': 3, 'prompt_len': 15, 'n_token_sample': 9, 'n_token_consider': 32000, 'mixing_method': 'sample_new_weights_with_score', 'smoothing': 'geom', 'sample_tokens': 0, 'sample_beams': 0, 'i_weights': [0.01, 0.04, 0.15, 0.18, 0.12], 'i_length': [1, 2, 3, 4, 5]}\n"
     ]
    }
   ],
   "source": [
    "# Params (default parameters for all cases)\n",
    "param_file = \"./params/p15_d3_mixed.json\"\n",
    "with open(param_file, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "    print(f\"Parameters: {params}\")\n",
    "alpha = params[\"alpha\"]\n",
    "temp = params[\"temp\"]\n",
    "n_drafts = params[\"n_drafts\"]\n",
    "prompt_len = params[\"prompt_len\"]\n",
    "n_token_sample = params[\"n_token_sample\"]\n",
    "i_weights = params[\"i_weights\"]\n",
    "i_length = params[\"i_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bigram...\n",
      "1310800\n",
      "Making trigram...\n",
      "671088728\n",
      "Making fourgram...\n",
      "2684354648\n",
      "Making fivegram...\n",
      "5368709200\n",
      "Making sixgram...\n",
      "5368709200\n"
     ]
    }
   ],
   "source": [
    "# Create ngram models\n",
    "ngrams = make_models(\"./ckpts-200k\", bigram=True, trigram=True, fourgram=True, fivegram=True, sixgram=True, sevengram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_device = torch.device(\"cuda:0\")\n",
    "reg_device = torch.device(\"cuda:1\")\n",
    "tokenizer = Tokenizer('./7B/tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = \"10302\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/raivn/ethans/miniconda3/envs/llms_12.1/lib/python3.11/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 18.50 seconds\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "weight_path = \"./7B/\"\n",
    "model = SuperposedLlama.build(ckpt_dir=weight_path, \n",
    "                         tokenizer_path='./7B/tokenizer.model', \n",
    "                         max_seq_len=100, \n",
    "                         max_batch_size=32,\n",
    "                         device=sup_device,\n",
    "                         model_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:35<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "sup_sequences, sup_ppl = evaluate_mixed_losses(data=dataset,\n",
    "                                                   model=model,\n",
    "                                                   smoothing=\"geom\",\n",
    "                                                   tokenizer=tokenizer,\n",
    "                                                   prompt_len=prompt_len,\n",
    "                                                   max_gen_len=10,\n",
    "                                                   alpha=alpha,\n",
    "                                                   temp=temp,\n",
    "                                                   n_drafts=n_drafts,\n",
    "                                                   n_token_sample=n_token_sample,\n",
    "                                                   bsz=32,\n",
    "                                                   i_weights=i_weights,\n",
    "                                                   i_length=i_length,\n",
    "                                                   ngrams=ngrams,\n",
    "                                                   get_time=False,\n",
    "                                                   penalty=200,\n",
    "                                                   marker=True)\n",
    "finish_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:03:48.817127, Average Time: 0:00:00.045763\n"
     ]
    }
   ],
   "source": [
    "duration = finish_time - start_time\n",
    "print(f\"Time: {duration}, Average Time: {duration / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results into file. \n",
    "# grader.py and diversity_grader.py use this file for perplexity evaluation.\n",
    "file_name = \"\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "    pickle.dump(sup_sequences, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nucleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loaded in 7.42 seconds\n"
     ]
    }
   ],
   "source": [
    "reg_model = Llama.build(ckpt_dir=\"./7B/\", \n",
    "                    tokenizer_path='./7B/tokenizer.model', \n",
    "                    max_seq_len=100, \n",
    "                    max_batch_size=32,\n",
    "                    device=reg_device,\n",
    "                    model_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:43<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:00:43.230412, Average Time: 0:00:00.008646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "nucleus_sequences, nucleus_ppl = evaluate_nucleus_losses(data=dataset,\n",
    "                                       model=reg_model,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       prompt_len=prompt_len,\n",
    "                                       max_gen_len=10,\n",
    "                                       temp=0.6,\n",
    "                                       bsz=32)\n",
    "finish_time = datetime.now() \n",
    "duration = finish_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:00:43.230412, Average Time: 0:00:00.008646\n"
     ]
    }
   ],
   "source": [
    "nucleus_sequences = nucleus_sequences.reshape(len(dataset), 1, -1)\n",
    "print(f\"Time: {duration}, Average Time: {duration / len(dataset)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokenizer, encoding):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tokenizer (Any): Tokenizer\n",
    "        encoding (torch.Tensor): Encoding\n",
    "    Returns:\n",
    "        decoding (str)\n",
    "    \"\"\"\n",
    "    eos_locs = (encoding == tokenizer.eos_id).nonzero()\n",
    "    if len(eos_locs > 0):\n",
    "        encoding = encoding[:eos_locs[0]]\n",
    "    return tokenizer.decode(encoding.to(torch.int32).tolist())\n",
    "    \n",
    "def print_results(tokenizer, predictions, n_drafts=n_drafts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tokenizer (Any): Tokenizer\n",
    "        predictions (torch.Tensor): Tokens of predicted sequences, flattened to (n_prompts * n_drafts, gen_len)\n",
    "    Returns:\n",
    "        Mauve score\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for i in tqdm(range(len(predictions))):\n",
    "        d = decode(tokenizer, predictions[i])\n",
    "        if i <= 15:\n",
    "            # first draft of this prompt\n",
    "            if i % n_drafts == 0:\n",
    "                count = 0\n",
    "                print(\"---------------\")\n",
    "                prompt = decode(tokenizer, predictions[i][:prompt_len])\n",
    "                print(f\"prompt: {prompt}\")\n",
    "            print(f\"{count}: {d}\")\n",
    "            count += 1\n",
    "        else: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                           | 16/1500 [00:00<00:00, 4323.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "prompt: Is this restaurant family-friendly ? Yes No Unsure\n",
      "\n",
      "0: Is this restaurant family-friendly ? Yes No Unsure\n",
      "I'm a big fan of the food and\n",
      "1: Is this restaurant family-friendly ? Yes No Unsure\n",
      "I'm a big fan of the food,\n",
      "2: Is this restaurant family-friendly ? Yes No Unsure\n",
      "I'm a big fan of the food here\n",
      "---------------\n",
      "prompt: Clinton talks about her time of 'reflection' during sick\n",
      "0: Clinton talks about her time of 'reflection' during sick leave\n",
      "Clinton talks about her time of\n",
      "1: Clinton talks about her time of 'reflection' during sickness\n",
      "Clinton talks about her time of\n",
      "2: Clinton talks about her time of 'reflection' during sick leave\n",
      "Clinton talks about her health of\n",
      "---------------\n",
      "prompt: House Majority Whip Steve Scalise has been discharged\n",
      "0: House Majority Whip Steve Scalise has been discharged from the hospital after being shot at a congression\n",
      "1: House Majority Whip Steve Scalise has been discharged from the hospital after being shot in a congression\n",
      "2: House Majority Whip Steve Scalise has been discharged from the hospital after being shot during a congression\n",
      "---------------\n",
      "prompt: Insight Course: Lesson 14\n",
      "\n",
      "Control of\n",
      "0: Insight Course: Lesson 14\n",
      "\n",
      "Control of the Mind\n",
      "\n",
      "The mind is the most important\n",
      "1: Insight Course: Lesson 14\n",
      "\n",
      "Control of the Mind\n",
      "\n",
      "The mind is the most powerful\n",
      "2: Insight Course: Lesson 14\n",
      "\n",
      "Control of the Mind\n",
      "\n",
      "The mind is a most important\n",
      "---------------\n",
      "prompt: BY JENNIE MCNULTY\n",
      "\n",
      "Lesbian.\n",
      "0: BY JENNIE MCNULTY\n",
      "\n",
      "Lesbian.\n",
      "\n",
      "BY JENNIE MCNULT\n",
      "1: BY JENNIE MCNULTY\n",
      "\n",
      "Lesbian.\n",
      "\n",
      "A JENNIE MCNULT\n",
      "2: BY JENNIE MCNULTY\n",
      "\n",
      "Lesbian.\n",
      "\n",
      "BY MENNIE MCNULT\n",
      "---------------\n",
      "prompt: The Buddha's Teaching As It Is\n",
      "\n",
      "In\n",
      "0: The Buddha's Teaching As It Is\n",
      "\n",
      "In the Buddha's teaching, there is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(tokenizer, predictions=sup_sequences.reshape(len(dataset) * n_drafts, -1), n_drafts=n_drafts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                           | 16/5000 [00:00<00:01, 2567.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "prompt: Is this restaurant family-friendly ? Yes No Unsure\n",
      "\n",
      "0: Is this restaurant family-friendly ? Yes No Unsure\n",
      "10160 Cedar Ave\n",
      "---------------\n",
      "prompt: Clinton talks about her time of 'reflection' during sick\n",
      "0: Clinton talks about her time of 'reflection' during sickness\n",
      "Clinton talks about her time of\n",
      "---------------\n",
      "prompt: House Majority Whip Steve Scalise has been discharged\n",
      "0: House Majority Whip Steve Scalise has been discharged from the hospital, his office announced Wednesday\n",
      "---------------\n",
      "prompt: Insight Course: Lesson 14\n",
      "\n",
      "Control of\n",
      "0: Insight Course: Lesson 14\n",
      "\n",
      "Control of the Body\n",
      "\n",
      "<p align=\"right\">\n",
      "---------------\n",
      "prompt: BY JENNIE MCNULTY\n",
      "\n",
      "Lesbian.\n",
      "0: BY JENNIE MCNULTY\n",
      "\n",
      "Lesbian. Bisexual. Queer. Transgender.\n",
      "---------------\n",
      "prompt: The Buddha's Teaching As It Is\n",
      "\n",
      "In\n",
      "0: The Buddha's Teaching As It Is\n",
      "\n",
      "In the year 1962, I had\n",
      "---------------\n",
      "prompt: As part of a broad initiative to combat sexual harassment and\n",
      "0: As part of a broad initiative to combat sexual harassment and assault on college campuses, the federal government is\n",
      "---------------\n",
      "prompt: The Atlanta Falcons have started the 2015 season \n",
      "0: The Atlanta Falcons have started the 2015 season 3-0, but they have a lot of\n",
      "---------------\n",
      "prompt: Front Page Torrents Favorites My Home My Galleries Top\n",
      "0: Front Page Torrents Favorites My Home My Galleries Topic Tools\n",
      "How to use the Torrents\n",
      "---------------\n",
      "prompt: They have changed the phone menu to try to deflect us to email\n",
      "0: They have changed the phone menu to try to deflect us to email, but the phone menu is still working.\n",
      "\n",
      "---------------\n",
      "prompt: One Page\n",
      "\n",
      "One Page is a browser extension for automatically displaying multi\n",
      "0: One Page\n",
      "\n",
      "One Page is a browser extension for automatically displaying multi-page articles in one page.\n",
      "\n",
      "##\n",
      "---------------\n",
      "prompt: Intro \"In his search for food, early man tried all kinds\n",
      "0: Intro \"In his search for food, early man tried all kinds of things. They tried to eat the plants and\n",
      "---------------\n",
      "prompt: Having trouble viewing the video? Try disabling any ad blocking extensions\n",
      "0: Having trouble viewing the video? Try disabling any ad blocking extensions in your browser.\n",
      "The Birth of the\n",
      "---------------\n",
      "prompt: Get Liverpool FC updates directly to your inbox Subscribe Thank you for\n",
      "0: Get Liverpool FC updates directly to your inbox Subscribe Thank you for subscribingWe have more newslettersShow me\n",
      "---------------\n",
      "prompt: Super Mario Run will be available on Android devices beginning in March, N\n",
      "0: Super Mario Run will be available on Android devices beginning in March, Nintendo announced in a blog post on Monday.\n",
      "\n",
      "---------------\n",
      "prompt: The California-based electric car manufacturer joins Jaguar, Land\n",
      "0: The California-based electric car manufacturer joins Jaguar, Land Rover, and BMW in making a significant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(tokenizer, predictions=nucleus_sequences.reshape(len(dataset) * 1, -1), n_drafts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
